{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: ultralytics 8.0.43\n",
      "Uninstalling ultralytics-8.0.43:\n",
      "  Successfully uninstalled ultralytics-8.0.43\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall ultralytics -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files removed: 16\n"
     ]
    }
   ],
   "source": [
    "!pip cache purge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting ultralytics\n",
      "  Downloading ultralytics-8.2.42-py3-none-any.whl.metadata (41 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.2/41.2 kB\u001b[0m \u001b[31m932.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy<2.0.0,>=1.23.5 in /home/pande/.local/lib/python3.9/site-packages (from ultralytics) (1.26.4)\n",
      "Requirement already satisfied: matplotlib>=3.3.0 in /home/pande/.local/lib/python3.9/site-packages (from ultralytics) (3.8.3)\n",
      "Requirement already satisfied: opencv-python>=4.6.0 in /home/pande/.local/lib/python3.9/site-packages (from ultralytics) (4.9.0.80)\n",
      "Requirement already satisfied: pillow>=7.1.2 in /home/pande/.local/lib/python3.9/site-packages (from ultralytics) (9.5.0)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in /home/pande/.local/lib/python3.9/site-packages (from ultralytics) (6.0.1)\n",
      "Requirement already satisfied: requests>=2.23.0 in /home/pande/.local/lib/python3.9/site-packages (from ultralytics) (2.28.2)\n",
      "Requirement already satisfied: scipy>=1.4.1 in /home/pande/.local/lib/python3.9/site-packages (from ultralytics) (1.12.0)\n",
      "Requirement already satisfied: torch>=1.8.0 in /home/pande/.local/lib/python3.9/site-packages (from ultralytics) (2.2.1+cu118)\n",
      "Requirement already satisfied: torchvision>=0.9.0 in /home/pande/.local/lib/python3.9/site-packages (from ultralytics) (0.17.1+cu118)\n",
      "Requirement already satisfied: tqdm>=4.64.0 in /home/pande/.local/lib/python3.9/site-packages (from ultralytics) (4.65.2)\n",
      "Requirement already satisfied: psutil in /home/pande/.local/lib/python3.9/site-packages (from ultralytics) (5.9.8)\n",
      "Requirement already satisfied: py-cpuinfo in /home/pande/.local/lib/python3.9/site-packages (from ultralytics) (9.0.0)\n",
      "Requirement already satisfied: pandas>=1.1.4 in /home/pande/.local/lib/python3.9/site-packages (from ultralytics) (2.2.1)\n",
      "Requirement already satisfied: seaborn>=0.11.0 in /home/pande/.local/lib/python3.9/site-packages (from ultralytics) (0.13.2)\n",
      "Requirement already satisfied: ultralytics-thop>=2.0.0 in /home/pande/.local/lib/python3.9/site-packages (from ultralytics) (2.0.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/pande/.local/lib/python3.9/site-packages (from matplotlib>=3.3.0->ultralytics) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/pande/.local/lib/python3.9/site-packages (from matplotlib>=3.3.0->ultralytics) (0.10.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/pande/.local/lib/python3.9/site-packages (from matplotlib>=3.3.0->ultralytics) (4.49.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/pande/.local/lib/python3.9/site-packages (from matplotlib>=3.3.0->ultralytics) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/pande/.local/lib/python3.9/site-packages (from matplotlib>=3.3.0->ultralytics) (23.2)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/pande/.local/lib/python3.9/site-packages (from matplotlib>=3.3.0->ultralytics) (3.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/pande/.local/lib/python3.9/site-packages (from matplotlib>=3.3.0->ultralytics) (2.8.2)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in /home/pande/.local/lib/python3.9/site-packages (from matplotlib>=3.3.0->ultralytics) (6.1.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/pande/.local/lib/python3.9/site-packages (from pandas>=1.1.4->ultralytics) (2023.4)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/pande/.local/lib/python3.9/site-packages (from pandas>=1.1.4->ultralytics) (2024.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/pande/.local/lib/python3.9/site-packages (from requests>=2.23.0->ultralytics) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests>=2.23.0->ultralytics) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/pande/.local/lib/python3.9/site-packages (from requests>=2.23.0->ultralytics) (1.26.19)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/pande/.local/lib/python3.9/site-packages (from requests>=2.23.0->ultralytics) (2023.7.22)\n",
      "Requirement already satisfied: filelock in /home/pande/.local/lib/python3.9/site-packages (from torch>=1.8.0->ultralytics) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/pande/.local/lib/python3.9/site-packages (from torch>=1.8.0->ultralytics) (4.10.0)\n",
      "Requirement already satisfied: sympy in /home/pande/.local/lib/python3.9/site-packages (from torch>=1.8.0->ultralytics) (1.12)\n",
      "Requirement already satisfied: networkx in /home/pande/.local/lib/python3.9/site-packages (from torch>=1.8.0->ultralytics) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /home/pande/.local/lib/python3.9/site-packages (from torch>=1.8.0->ultralytics) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /home/pande/.local/lib/python3.9/site-packages (from torch>=1.8.0->ultralytics) (2023.4.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.8.89 in /home/pande/.local/lib/python3.9/site-packages (from torch>=1.8.0->ultralytics) (11.8.89)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.8.89 in /home/pande/.local/lib/python3.9/site-packages (from torch>=1.8.0->ultralytics) (11.8.89)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.8.87 in /home/pande/.local/lib/python3.9/site-packages (from torch>=1.8.0->ultralytics) (11.8.87)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.7.0.84 in /home/pande/.local/lib/python3.9/site-packages (from torch>=1.8.0->ultralytics) (8.7.0.84)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.11.3.6 in /home/pande/.local/lib/python3.9/site-packages (from torch>=1.8.0->ultralytics) (11.11.3.6)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /home/pande/.local/lib/python3.9/site-packages (from torch>=1.8.0->ultralytics) (10.9.0.58)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.3.0.86 in /home/pande/.local/lib/python3.9/site-packages (from torch>=1.8.0->ultralytics) (10.3.0.86)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.1.48 in /home/pande/.local/lib/python3.9/site-packages (from torch>=1.8.0->ultralytics) (11.4.1.48)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.5.86 in /home/pande/.local/lib/python3.9/site-packages (from torch>=1.8.0->ultralytics) (11.7.5.86)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.19.3 in /home/pande/.local/lib/python3.9/site-packages (from torch>=1.8.0->ultralytics) (2.19.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.8.86 in /home/pande/.local/lib/python3.9/site-packages (from torch>=1.8.0->ultralytics) (11.8.86)\n",
      "Requirement already satisfied: triton==2.2.0 in /home/pande/.local/lib/python3.9/site-packages (from torch>=1.8.0->ultralytics) (2.2.0)\n",
      "Requirement already satisfied: six in /usr/lib/python3/dist-packages (from cycler>=0.10->matplotlib>=3.3.0->ultralytics) (1.16.0)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /home/pande/.local/lib/python3.9/site-packages (from importlib-resources>=3.2.0->matplotlib>=3.3.0->ultralytics) (3.17.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/pande/.local/lib/python3.9/site-packages (from jinja2->torch>=1.8.0->ultralytics) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/pande/.local/lib/python3.9/site-packages (from sympy->torch>=1.8.0->ultralytics) (1.3.0)\n",
      "Downloading ultralytics-8.2.42-py3-none-any.whl (792 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m793.0/793.0 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: ultralytics\n",
      "Successfully installed ultralytics-8.2.42\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/usr/bin/python3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install ultralytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/usr/bin/python3 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/usr/bin/python3 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q \"openvino>=2023.1.0\" \"nncf>=2.5.0\"\n",
    "%pip install -q \"torch>=2.1\" \"torchvision>=0.16\" \"ultralytics\" onnx --extra-index-url https://download.pytorch.org/whl/cpu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, Dict\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from ultralytics.utils.plotting import colors\n",
    "\n",
    "\n",
    "def plot_one_box(box:np.ndarray, img:np.ndarray, color:Tuple[int, int, int] = None, mask:np.ndarray = None, label:str = None, line_thickness:int = 5):\n",
    "    \"\"\"\n",
    "    Helper function for drawing single bounding box on image\n",
    "    Parameters:\n",
    "        x (np.ndarray): bounding box coordinates in format [x1, y1, x2, y2]\n",
    "        img (no.ndarray): input image\n",
    "        color (Tuple[int, int, int], *optional*, None): color in BGR format for drawing box, if not specified will be selected randomly\n",
    "        mask (np.ndarray, *optional*, None): instance segmentation mask polygon in format [N, 2], where N - number of points in contour, if not provided, only box will be drawn\n",
    "        label (str, *optonal*, None): box label string, if not provided will not be provided as drowing result\n",
    "        line_thickness (int, *optional*, 5): thickness for box drawing lines\n",
    "    \"\"\"\n",
    "    # Plots one bounding box on image img\n",
    "    tl = line_thickness or round(0.002 * (img.shape[0] + img.shape[1]) / 2) + 1  # line/font thickness\n",
    "    color = color or [random.randint(0, 255) for _ in range(3)]\n",
    "    c1, c2 = (int(box[0]), int(box[1])), (int(box[2]), int(box[3]))\n",
    "    cv2.rectangle(img, c1, c2, color, thickness=tl, lineType=cv2.LINE_AA)\n",
    "    if label:\n",
    "        tf = max(tl - 1, 1)  # font thickness\n",
    "        t_size = cv2.getTextSize(label, 0, fontScale=tl / 3, thickness=tf)[0]\n",
    "        c2 = c1[0] + t_size[0], c1[1] - t_size[1] - 3\n",
    "        cv2.rectangle(img, c1, c2, color, -1, cv2.LINE_AA)  # filled\n",
    "        cv2.putText(img, label, (c1[0], c1[1] - 2), 0, tl / 3, [225, 255, 255], thickness=tf, lineType=cv2.LINE_AA)\n",
    "    if mask is not None:\n",
    "        image_with_mask = img.copy()\n",
    "        mask\n",
    "        cv2.fillPoly(image_with_mask, pts=[mask.astype(int)], color=color)\n",
    "        img = cv2.addWeighted(img, 0.5, image_with_mask, 0.5, 1)\n",
    "    return img\n",
    "\n",
    "\n",
    "def draw_results(results:Dict, source_image:np.ndarray, label_map:Dict):\n",
    "    \"\"\"\n",
    "    Helper function for drawing bounding boxes on image\n",
    "    Parameters:\n",
    "        image_res (np.ndarray): detection predictions in format [x1, y1, x2, y2, score, label_id]\n",
    "        source_image (np.ndarray): input image for drawing\n",
    "        label_map; (Dict[int, str]): label_id to class name mapping\n",
    "    Returns:\n",
    "        \n",
    "    \"\"\"\n",
    "    boxes = results[\"det\"]\n",
    "    masks = results.get(\"segment\")\n",
    "    for idx, (*xyxy, conf, lbl) in enumerate(boxes):\n",
    "        label = f'{label_map[int(lbl)]} {conf:.2f}'\n",
    "        mask = masks[idx] if masks is not None else None\n",
    "        source_image = plot_one_box(xyxy, source_image, mask=mask, label=label, color=colors(int(lbl)), line_thickness=1)\n",
    "    return source_image\n",
    "\n",
    "def draw_results(results:Dict, source_image:np.ndarray, label_map:Dict):\n",
    "    \"\"\"\n",
    "    Helper function for drawing bounding boxes on image\n",
    "    Parameters:\n",
    "        image_res (np.ndarray): detection predictions in format [x1, y1, x2, y2, score, label_id]\n",
    "        source_image (np.ndarray): input image for drawing\n",
    "        label_map; (Dict[int, str]): label_id to class name mapping\n",
    "    Returns:\n",
    "\n",
    "    \"\"\"\n",
    "    boxes = results[\"det\"]\n",
    "    masks = results.get(\"segment\")\n",
    "    h, w = source_image.shape[:2]\n",
    "    for idx, (*xyxy, conf, lbl) in enumerate(boxes):\n",
    "        label = f'{label_map[int(lbl)]} {conf:.2f}'\n",
    "        mask = masks[idx] if masks is not None else None\n",
    "        source_image = plot_one_box(xyxy, source_image, mask=mask, label=label, color=colors(int(lbl)), line_thickness=1)\n",
    "    return source_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_PATH = \"/home/pande/seg-kompress/Kompress/model/sponge_mp4-0034_jpg.rf.1fb3b9e0a3b839ba3f55df1407f11220.jpg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "# DET_MODEL_NAME = \"/home/pande/seg-kompress/Kompress/ipynb_checkpoints/best.pt\"\n",
    "\n",
    "seg_model = YOLO('/home/pande/seg-kompress/Kompress/ipynb_checkpoints/best.pt')\n",
    "label_map = seg_model.model.names\n",
    "# res = seg_model(IMAGE_PATH)\n",
    "# Image.fromarray(res[0].plot()[:, :, ::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# instance segmentation model\n",
    "seg_model_path = Path(\"/home/pande/seg-kompress/Kompress/ipynb_checkpoints/best_openvino_model/best.xml\")\n",
    "if not seg_model_path.exists():\n",
    "    seg_model.export(format=\"openvino\", dynamic=True, half=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "from ultralytics.utils import ops\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def letterbox(img: np.ndarray, new_shape:Tuple[int, int] = (640, 640), color:Tuple[int, int, int] = (114, 114, 114), auto:bool = False, scale_fill:bool = False, scaleup:bool = False, stride:int = 32):\n",
    "    \"\"\"\n",
    "    Resize image and padding for detection. Takes image as input, \n",
    "    resizes image to fit into new shape with saving original aspect ratio and pads it to meet stride-multiple constraints\n",
    "    \n",
    "    Parameters:\n",
    "      img (np.ndarray): image for preprocessing\n",
    "      new_shape (Tuple(int, int)): image size after preprocessing in format [height, width]\n",
    "      color (Tuple(int, int, int)): color for filling padded area\n",
    "      auto (bool): use dynamic input size, only padding for stride constrins applied\n",
    "      scale_fill (bool): scale image to fill new_shape\n",
    "      scaleup (bool): allow scale image if it is lower then desired input size, can affect model accuracy\n",
    "      stride (int): input padding stride\n",
    "    Returns:\n",
    "      img (np.ndarray): image after preprocessing\n",
    "      ratio (Tuple(float, float)): hight and width scaling ratio\n",
    "      padding_size (Tuple(int, int)): height and width padding size\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    # Resize and pad image while meeting stride-multiple constraints\n",
    "    shape = img.shape[:2]  # current shape [height, width]\n",
    "    if isinstance(new_shape, int):\n",
    "        new_shape = (new_shape, new_shape)\n",
    "\n",
    "    # Scale ratio (new / old)\n",
    "    r = min(new_shape[0] / shape[0], new_shape[1] / shape[1])\n",
    "    if not scaleup:  # only scale down, do not scale up (for better test mAP)\n",
    "        r = min(r, 1.0)\n",
    "\n",
    "    # Compute padding\n",
    "    ratio = r, r  # width, height ratios\n",
    "    new_unpad = int(round(shape[1] * r)), int(round(shape[0] * r))\n",
    "    dw, dh = new_shape[1] - new_unpad[0], new_shape[0] - new_unpad[1]  # wh padding\n",
    "    if auto:  # minimum rectangle\n",
    "        dw, dh = np.mod(dw, stride), np.mod(dh, stride)  # wh padding\n",
    "    elif scale_fill:  # stretch\n",
    "        dw, dh = 0.0, 0.0\n",
    "        new_unpad = (new_shape[1], new_shape[0])\n",
    "        ratio = new_shape[1] / shape[1], new_shape[0] / shape[0]  # width, height ratios\n",
    "\n",
    "    dw /= 2  # divide padding into 2 sides\n",
    "    dh /= 2\n",
    "\n",
    "    if shape[::-1] != new_unpad:  # resize\n",
    "        img = cv2.resize(img, new_unpad, interpolation=cv2.INTER_LINEAR)\n",
    "    top, bottom = int(round(dh - 0.1)), int(round(dh + 0.1))\n",
    "    left, right = int(round(dw - 0.1)), int(round(dw + 0.1))\n",
    "    img = cv2.copyMakeBorder(img, top, bottom, left, right, cv2.BORDER_CONSTANT, value=color)  # add border\n",
    "    return img, ratio, (dw, dh)\n",
    "\n",
    "\n",
    "def preprocess_image(img0: np.ndarray):\n",
    "    \"\"\"\n",
    "    Preprocess image according to YOLOv8 input requirements. \n",
    "    Takes image in np.array format, resizes it to specific size using letterbox resize and changes data layout from HWC to CHW.\n",
    "    \n",
    "    Parameters:\n",
    "      img0 (np.ndarray): image for preprocessing\n",
    "    Returns:\n",
    "      img (np.ndarray): image after preprocessing\n",
    "    \"\"\"\n",
    "    # resize\n",
    "    img = letterbox(img0)[0]\n",
    "    \n",
    "    # Convert HWC to CHW\n",
    "    img = img.transpose(2, 0, 1)\n",
    "    img = np.ascontiguousarray(img)\n",
    "    return img\n",
    "\n",
    "\n",
    "def image_to_tensor(image:np.ndarray):\n",
    "    \"\"\"\n",
    "    Preprocess image according to YOLOv8 input requirements. \n",
    "    Takes image in np.array format, resizes it to specific size using letterbox resize and changes data layout from HWC to CHW.\n",
    "    \n",
    "    Parameters:\n",
    "      img (np.ndarray): image for preprocessing\n",
    "    Returns:\n",
    "      input_tensor (np.ndarray): input tensor in NCHW format with float32 values in [0, 1] range \n",
    "    \"\"\"\n",
    "    input_tensor = image.astype(np.float32)  # uint8 to fp32\n",
    "    input_tensor /= 255.0  # 0 - 255 to 0.0 - 1.0\n",
    "    \n",
    "    # add batch dimension\n",
    "    if input_tensor.ndim == 3:\n",
    "        input_tensor = np.expand_dims(input_tensor, 0)\n",
    "    return input_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    scale_segments = ops.scale_segments\n",
    "except AttributeError:\n",
    "    scale_segments = ops.scale_coords\n",
    "\n",
    "def postprocess(\n",
    "    pred_boxes:np.ndarray, \n",
    "    input_hw:Tuple[int, int], \n",
    "    orig_img:np.ndarray, \n",
    "    min_conf_threshold:float = 0.25, \n",
    "    nms_iou_threshold:float = 0.7, \n",
    "    agnosting_nms:bool = False, \n",
    "    max_detections:int = 300,\n",
    "    pred_masks:np.ndarray = None,\n",
    "    retina_mask:bool = False\n",
    "):\n",
    "    \"\"\"\n",
    "    YOLOv8 model postprocessing function. Applied non maximum supression algorithm to detections and rescale boxes to original image size\n",
    "    Parameters:\n",
    "        pred_boxes (np.ndarray): model output prediction boxes\n",
    "        input_hw (np.ndarray): preprocessed image\n",
    "        orig_image (np.ndarray): image before preprocessing\n",
    "        min_conf_threshold (float, *optional*, 0.25): minimal accepted confidence for object filtering\n",
    "        nms_iou_threshold (float, *optional*, 0.45): minimal overlap score for removing objects duplicates in NMS\n",
    "        agnostic_nms (bool, *optiona*, False): apply class agnostinc NMS approach or not\n",
    "        max_detections (int, *optional*, 300):  maximum detections after NMS\n",
    "        pred_masks (np.ndarray, *optional*, None): model ooutput prediction masks, if not provided only boxes will be postprocessed\n",
    "        retina_mask (bool, *optional*, False): retina mask postprocessing instead of native decoding\n",
    "    Returns:\n",
    "       pred (List[Dict[str, np.ndarray]]): list of dictionary with det - detected boxes in format [x1, y1, x2, y2, score, label] and\n",
    "                                           segment - segmentation polygons for each element in batch\n",
    "    \"\"\"\n",
    "    nms_kwargs = {\"agnostic\": agnosting_nms, \"max_det\":max_detections}\n",
    "    # if pred_masks is not None:\n",
    "    #     nms_kwargs[\"nm\"] = 32\n",
    "    preds = ops.non_max_suppression(\n",
    "        torch.from_numpy(pred_boxes),\n",
    "        min_conf_threshold,\n",
    "        nms_iou_threshold,\n",
    "        **nms_kwargs\n",
    "    )\n",
    "    results = []\n",
    "    proto = torch.from_numpy(pred_masks) if pred_masks is not None else None\n",
    "\n",
    "    for i, pred in enumerate(preds):\n",
    "        print(f\"Shape of proto: {proto.shape if proto is not None else 'None'}\")\n",
    "        print(f\"Shape of pred: {pred.shape}\")\n",
    "        print(f\"Shape of pred[:, 6:]: {pred[:, 6:].shape}\")\n",
    "        shape = orig_img[i].shape if isinstance(orig_img, list) else orig_img.shape\n",
    "        if not len(pred):\n",
    "            results.append({\"det\": [], \"segment\": []})\n",
    "            continue\n",
    "        if proto is None:\n",
    "            pred[:, :4] = ops.scale_boxes(input_hw, pred[:, :4], shape).round()\n",
    "            results.append({\"det\": pred})\n",
    "            continue\n",
    "        if retina_mask:\n",
    "            pred[:, :4] = ops.scale_boxes(input_hw, pred[:, :4], shape).round()\n",
    "            masks = ops.process_mask_native(proto[i], pred[:, 6:], pred[:, :4], shape[:2])  # HWC\n",
    "            segments = [scale_segments(input_hw, x, shape, normalize=False) for x in ops.masks2segments(masks)]\n",
    "        else:\n",
    "            masks = ops.process_mask(proto[i], pred[:, 6:], pred[:, :4], input_hw, upsample=True)\n",
    "            pred[:, :4] = ops.scale_boxes(input_hw, pred[:, :4], shape).round()\n",
    "            segments = [scale_segments(input_hw, x, shape, normalize=False) for x in ops.masks2segments(masks)]\n",
    "        results.append({\"det\": pred[:, :6].numpy(), \"segment\": segments})\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da7760fd07fd408e9f48904a76add306",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Device:', index=2, options=('CPU', 'GPU', 'AUTO'), value='AUTO')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "import openvino as ov \n",
    "\n",
    "core = ov.Core()\n",
    "\n",
    "device = widgets.Dropdown(\n",
    "    options=core.available_devices + [\"AUTO\"],\n",
    "    value='AUTO',\n",
    "    description='Device:',\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of proto: torch.Size([1, 32, 160, 160])\n",
      "Shape of pred: torch.Size([300, 6])\n",
      "Shape of pred[:, 6:]: torch.Size([300, 0])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (300x0 and 32x25600)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 35\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m detections\n\u001b[1;32m     34\u001b[0m input_image \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(Image\u001b[38;5;241m.\u001b[39mopen(IMAGE_PATH))\n\u001b[0;32m---> 35\u001b[0m detections \u001b[38;5;241m=\u001b[39m \u001b[43mdetect\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_image\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseg_compiled_model\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     36\u001b[0m image_with_masks \u001b[38;5;241m=\u001b[39m draw_results(detections, input_image, label_map)\n\u001b[1;32m     38\u001b[0m Image\u001b[38;5;241m.\u001b[39mfromarray(image_with_masks)\n",
      "Cell \u001b[0;32mIn[28], line 31\u001b[0m, in \u001b[0;36mdetect\u001b[0;34m(image, model)\u001b[0m\n\u001b[1;32m     29\u001b[0m     masks \u001b[38;5;241m=\u001b[39m result[model\u001b[38;5;241m.\u001b[39moutput(\u001b[38;5;241m1\u001b[39m)]\n\u001b[1;32m     30\u001b[0m input_hw \u001b[38;5;241m=\u001b[39m input_tensor\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m:]\n\u001b[0;32m---> 31\u001b[0m detections \u001b[38;5;241m=\u001b[39m \u001b[43mpostprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpred_boxes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mboxes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_hw\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_hw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morig_img\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpred_masks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmasks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m detections\n",
      "Cell \u001b[0;32mIn[26], line 62\u001b[0m, in \u001b[0;36mpostprocess\u001b[0;34m(pred_boxes, input_hw, orig_img, min_conf_threshold, nms_iou_threshold, agnosting_nms, max_detections, pred_masks, retina_mask)\u001b[0m\n\u001b[1;32m     60\u001b[0m     segments \u001b[38;5;241m=\u001b[39m [scale_segments(input_hw, x, shape, normalize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mmasks2segments(masks)]\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 62\u001b[0m     masks \u001b[38;5;241m=\u001b[39m \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess_mask\u001b[49m\u001b[43m(\u001b[49m\u001b[43mproto\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpred\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m6\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpred\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_hw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mupsample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m     pred[:, :\u001b[38;5;241m4\u001b[39m] \u001b[38;5;241m=\u001b[39m ops\u001b[38;5;241m.\u001b[39mscale_boxes(input_hw, pred[:, :\u001b[38;5;241m4\u001b[39m], shape)\u001b[38;5;241m.\u001b[39mround()\n\u001b[1;32m     64\u001b[0m     segments \u001b[38;5;241m=\u001b[39m [scale_segments(input_hw, x, shape, normalize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mmasks2segments(masks)]\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/ultralytics/utils/ops.py:688\u001b[0m, in \u001b[0;36mprocess_mask\u001b[0;34m(protos, masks_in, bboxes, shape, upsample)\u001b[0m\n\u001b[1;32m    686\u001b[0m c, mh, mw \u001b[38;5;241m=\u001b[39m protos\u001b[38;5;241m.\u001b[39mshape  \u001b[38;5;66;03m# CHW\u001b[39;00m\n\u001b[1;32m    687\u001b[0m ih, iw \u001b[38;5;241m=\u001b[39m shape\n\u001b[0;32m--> 688\u001b[0m masks \u001b[38;5;241m=\u001b[39m (\u001b[43mmasks_in\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m@\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mprotos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[43mc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m)\u001b[38;5;241m.\u001b[39msigmoid()\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, mh, mw)  \u001b[38;5;66;03m# CHW\u001b[39;00m\n\u001b[1;32m    689\u001b[0m width_ratio \u001b[38;5;241m=\u001b[39m mw \u001b[38;5;241m/\u001b[39m iw\n\u001b[1;32m    690\u001b[0m height_ratio \u001b[38;5;241m=\u001b[39m mh \u001b[38;5;241m/\u001b[39m ih\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (300x0 and 32x25600)"
     ]
    }
   ],
   "source": [
    "from openvino.runtime import Core, Model\n",
    "\n",
    "core = ov.Core()\n",
    "seg_ov_model = core.read_model(seg_model_path)\n",
    "if device.value != \"CPU\":\n",
    "    seg_ov_model.reshape({0: [1, 3, 640, 640]})\n",
    "ov_config = {}\n",
    "if \"GPU\" in device.value or (\"AUTO\" in device.value and \"GPU\" in core.available_devices):\n",
    "    ov_config = {\"GPU_DISABLE_WINOGRAD_CONVOLUTION\": \"YES\"}\n",
    "seg_compiled_model = core.compile_model(seg_ov_model, device.value, ov_config)\n",
    "\n",
    "\n",
    "def detect(image:np.ndarray, model:ov.Model):\n",
    "    \"\"\"\n",
    "    OpenVINO YOLOv8 model inference function. Preprocess image, runs model inference and postprocess results using NMS.\n",
    "    Parameters:\n",
    "        image (np.ndarray): input image.\n",
    "        model (Model): OpenVINO compiled model.\n",
    "    Returns:\n",
    "        detections (np.ndarray): detected boxes in format [x1, y1, x2, y2, score, label]\n",
    "    \"\"\"\n",
    "    num_outputs = len(model.outputs)\n",
    "    preprocessed_image = preprocess_image(image)\n",
    "    input_tensor = image_to_tensor(preprocessed_image)\n",
    "    result = model(input_tensor)\n",
    "    boxes = result[model.output(0)]\n",
    "    masks = None\n",
    "    if num_outputs > 1:\n",
    "        masks = result[model.output(1)]\n",
    "    input_hw = input_tensor.shape[2:]\n",
    "    detections = postprocess(pred_boxes=boxes, input_hw=input_hw, orig_img=image, pred_masks=masks)\n",
    "    return detections\n",
    "\n",
    "input_image = np.array(Image.open(IMAGE_PATH))\n",
    "detections = detect(input_image, seg_compiled_model)[0]\n",
    "image_with_masks = draw_results(detections, input_image, label_map)\n",
    "\n",
    "Image.fromarray(image_with_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics.utils.metrics import ConfusionMatrix\n",
    "\n",
    "\n",
    "def test(model:Model, core:Core, data_loader:torch.utils.data.DataLoader, validator, num_samples:int = None):\n",
    "    \"\"\"\n",
    "    OpenVINO YOLOv8 model accuracy validation function. Runs model validation on dataset and returns metrics\n",
    "    Parameters:\n",
    "        model (Model): OpenVINO model\n",
    "        data_loader (torch.utils.data.DataLoader): dataset loader\n",
    "        validato: instalce of validator class\n",
    "        num_samples (int, *optional*, None): validate model only on specified number samples, if provided\n",
    "    Returns:\n",
    "        stats: (Dict[str, float]) - dictionary with aggregated accuracy metrics statistics, key is metric name, value is metric value\n",
    "    \"\"\"\n",
    "    validator.seen = 0\n",
    "    validator.jdict = []\n",
    "    validator.stats = []\n",
    "    validator.batch_i = 1\n",
    "    validator.confusion_matrix = ConfusionMatrix(nc=validator.nc)\n",
    "    model.reshape({0: [1, 3, -1, -1]})\n",
    "    num_outputs = len(model.outputs)\n",
    "    compiled_model = core.compile_model(model)\n",
    "    for batch_i, batch in enumerate(data_loader):\n",
    "        if num_samples is not None and batch_i == num_samples:\n",
    "            break\n",
    "        batch = validator.preprocess(batch)\n",
    "        results = compiled_model(batch[\"img\"])\n",
    "        if num_outputs == 1:\n",
    "            preds = torch.from_numpy(results[compiled_model.output(0)])\n",
    "        else:\n",
    "            preds = [torch.from_numpy(results[compiled_model.output(0)]), torch.from_numpy(results[compiled_model.output(1)])]\n",
    "        preds = validator.postprocess(preds)\n",
    "        validator.update_metrics(preds, batch)\n",
    "    stats = validator.get_stats()\n",
    "    return stats\n",
    "\n",
    "\n",
    "def print_stats(stats:np.ndarray, total_images:int, total_objects:int):\n",
    "    \"\"\"\n",
    "    Helper function for printing accuracy statistic\n",
    "    Parameters:\n",
    "        stats: (Dict[str, float]) - dictionary with aggregated accuracy metrics statistics, key is metric name, value is metric value\n",
    "        total_images (int) -  number of evaluated images\n",
    "        total objects (int)\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    print(\"Boxes:\")\n",
    "    mp, mr, map50, mean_ap = stats['metrics/precision(B)'], stats['metrics/recall(B)'], stats['metrics/mAP50(B)'], stats['metrics/mAP50-95(B)']\n",
    "    # Print results\n",
    "    s = ('%20s' + '%12s' * 6) % ('Class', 'Images', 'Labels', 'Precision', 'Recall', 'mAP@.5', 'mAP@.5:.95')\n",
    "    print(s)\n",
    "    pf = '%20s' + '%12i' * 2 + '%12.3g' * 4  # print format\n",
    "    print(pf % ('all', total_images, total_objects, mp, mr, map50, mean_ap))\n",
    "    if 'metrics/precision(M)' in stats:\n",
    "        s_mp, s_mr, s_map50, s_mean_ap = stats['metrics/precision(M)'], stats['metrics/recall(M)'], stats['metrics/mAP50(M)'], stats['metrics/mAP50-95(M)']\n",
    "        # Print results\n",
    "        s = ('%20s' + '%12s' * 6) % ('Class', 'Images', 'Labels', 'Precision', 'Recall', 'mAP@.5', 'mAP@.5:.95')\n",
    "        print(s)\n",
    "        pf = '%20s' + '%12i' * 2 + '%12.3g' * 4  # print format\n",
    "        print(pf % ('all', total_images, total_objects, s_mp, s_mr, s_map50, s_mean_ap))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantized_seg_model = \"/home/pande/seg-kompress/Kompress/ipynb_checkpoints/model.xml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "30",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m input_image \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(Image\u001b[38;5;241m.\u001b[39mopen(IMAGE_PATH))\n\u001b[1;32m      5\u001b[0m detections \u001b[38;5;241m=\u001b[39m detect(input_image, quantized_seg_compiled_model)[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m----> 6\u001b[0m image_with_masks \u001b[38;5;241m=\u001b[39m \u001b[43mdraw_results\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdetections\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_image\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m Image\u001b[38;5;241m.\u001b[39mfromarray(image_with_masks)\n",
      "Cell \u001b[0;32mIn[1], line 52\u001b[0m, in \u001b[0;36mdraw_results\u001b[0;34m(results, source_image, label_map)\u001b[0m\n\u001b[1;32m     50\u001b[0m h, w \u001b[38;5;241m=\u001b[39m source_image\u001b[38;5;241m.\u001b[39mshape[:\u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, (\u001b[38;5;241m*\u001b[39mxyxy, conf, lbl) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(boxes):\n\u001b[0;32m---> 52\u001b[0m     label \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabel_map[\u001b[38;5;28mint\u001b[39m(lbl)]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconf\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     53\u001b[0m     mask \u001b[38;5;241m=\u001b[39m masks[idx] \u001b[38;5;28;01mif\u001b[39;00m masks \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     54\u001b[0m     source_image \u001b[38;5;241m=\u001b[39m plot_one_box(xyxy, source_image, mask\u001b[38;5;241m=\u001b[39mmask, label\u001b[38;5;241m=\u001b[39mlabel, color\u001b[38;5;241m=\u001b[39mcolors(\u001b[38;5;28mint\u001b[39m(lbl)), line_thickness\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: 30"
     ]
    }
   ],
   "source": [
    "if device != \"CPU\":\n",
    "    quantized_seg_model.reshape({0, [1, 3, 640, 640]})\n",
    "quantized_seg_compiled_model = core.compile_model(quantized_seg_model, device)\n",
    "input_image = np.array(Image.open(IMAGE_PATH))\n",
    "detections = detect(input_image, quantized_seg_compiled_model)[0]\n",
    "image_with_masks = draw_results(detections, input_image, label_map)\n",
    "\n",
    "Image.fromarray(image_with_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
